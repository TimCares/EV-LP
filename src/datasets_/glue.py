"""
This module contains the implementation of the GLUE datasets.
"""
import torchtext
import torch
from .base_datasets import BaseDataset, TextMixin
import os
from .data_utils import write_data_into_jsonl
from utils import Modality
from typing import List, Dict, Any, Union, Tuple, Iterator
import shutil
from torchvision.datasets.utils import download_url
import pandas as pd
import zipfile
from transformers import PreTrainedTokenizer, PreTrainedTokenizerFast
from registries import register_dataset

class GLUE(TextMixin, BaseDataset):
    def __init__(
        self,
        data_path:os.PathLike,
        split:str,
        tokenizer:Union[PreTrainedTokenizer, PreTrainedTokenizerFast]=None,
        max_seq_len:int=512,
    ):
        """Base class for the General Language Understanding Evaluation (GLUE) datasets.

        Args:
            data_path (os.PathLike): The path where the data is stored.
            split (str): The split of the data. One of 'train', 'val', or 'test'.
            tokenizer (Union[PreTrainedTokenizer, PreTrainedTokenizerFast], optional): A tokenizer class that implements
                the Huggingface tokenizer API. Used to tokenize text data. Defaults to None.
                If None, then the BERT base uncased tokenizer will be used by default:
                BertTokenizer.from_pretrained("bert-base-uncased").
            max_seq_len (int, optional): The maximum sequence length of the tokenized text data. Defaults to 512.
        """        
        super().__init__(
            data_path=data_path,
            split=split,
            tokenizer=tokenizer,
            max_seq_len=max_seq_len,    
        )

        if self.index_exists():
            return

        # if the maximum sequence length in the (subtype) dataset is lower than the maximum sequence length,
        # then the maximum sequence length is set to the maximum sequence length in the (subtype) dataset
        # this will increase efficiency and reduce memory usage, as the sequences will be padded to an unnecessarily long length otherwise
        # this requires iterating over the complete dataset, in order to get the maximum sequence length of all sequences in the dataset
        # some datasets are too large to just iterate over, so sometimes self._get_max_length() will just return self.max_seq_len (default)
        self.num_tokens_upper_bound = self._get_max_length(iter(self._dataset(split=self.split)))
        if self.num_tokens_upper_bound > self.max_seq_len:
            self.log(f"Upper bound length: {self.num_tokens_upper_bound} is greater than the maximum sequence length: {self.max_seq_len}.")
            self.num_tokens_upper_bound = self.max_seq_len

        self.create_index()
        shutil.rmtree(f'{self.path_to_data}/datasets') # clean up data generated by torchtext, we have our own implementation

    def prepare_sentence_pair(self, sentence1:str, sentence2:str=None) -> Dict[str, List[int]]:
        """Encodes a pair of sentences into one sequence. The sequence will start with the [CLS] token,
        followed by the tokens of the first sentence,
        then the [SEP] token, then the tokens of the second sentence, and finally the [SEP] token.
        If just one sentence is provided, the sequence will be encoded as usual with singular sentences.

        Args:
            sentence1 (str): The first sentence.
            sentence2 (str, optional): The second sentence. Defaults to None.

        Returns:
            Dict[str, List[int]]: A dictionary containing the sequence of token IDs (key "input_ids"), the attention mask (key "attention_mask"),
                the token type IDs (key "token_type_ids"), and a boolean value indicating whether the sequence was truncated (key "trunc").
        """        
        result_dict = self.tokenizer.encode_plus(sentence1, sentence2, padding='max_length',
                                                 max_length=self.num_tokens_upper_bound,
                                                 truncation=True, return_attention_mask=True)
        
        # if a sentence pair is too long, it will be truncated
        # this may erase important semantic information, so it is important to check how many examples are truncated
        # -> if the number of tokens generated from encoding the sentence pair is greater than the maximum number of tokens, then
        # the sentence pair is truncated in the "encode_plus" call above
        trunc = len(self.tokenizer.encode(sentence1, sentence2)) > self.num_tokens_upper_bound
        result_dict['trunc'] = trunc
        return result_dict
            
    def get_index_files(self) -> Tuple[str]:
        """Returns a tuple of strings, where each string is the name of an index file containing the data
        for the split of the dataset.

        Returns:
            Tuple[str]: Tuple of strings, where each string is the name of an index file containing the data.
        """        
        (f'{self.split}.jsonl',)
    
    @property
    def modality(self) -> Modality:
        return Modality.TEXT
    
    def _dataset(self, split:str) -> torch.utils.data.Dataset:
        """Gets the dataset object of the specific GLUE task from torchtext, for the specified split.

        Args:
            split (str): The split of the data. One of 'train', 'dev', or 'test'.

        Returns:
            torch.utils.data.Dataset: The dataset object of the specific GLUE task.
        """        
        raise NotImplementedError()
    
    @property
    def data_dir(self) -> str:
        """
        Name of the directory in self.data_path where the data is stored.
        """        
        return self._dataset_name + '_glue'
    
    @property
    def _dataset_name(self):
        """
        The name of the GLUE dataset, i.e. the name of the specific GLUE task (e.g. "cola").
        """        
        raise NotImplementedError()

    def __getitem__(self, index):
        item = self.items[index]
        return item
    
    def _get_max_length(self, data_loader:Iterator) -> int:
        """Gets the maximum sequence length of the tokenized text data in the dataset.
        By default, this method just returns the maximum sequence length set in the constructor. Obviously, this is faster than
        iterating over the complete dataset.

        Args:
            data_loader (Iterator): An iterator over the dataset.

        Returns:
            int: The maximum sequence length of the tokenized text data in the dataset.
        """        
        return self.max_seq_len
    

@register_dataset(name='cola_glue')
class CoLA(GLUE):
    def _dataset(self, split:str) -> torch.utils.data.Dataset:
        return torchtext.datasets.CoLA(root=self.path_to_data, split=split)
    
    @property
    def _dataset_name(self):
        return 'cola'
    
    def _get_max_length(self, data_loader) -> int:
        # d[2] -> the sentence as a string
        # len(d[2]) -> the length of string
        # the latter is not the number of tokens, but the number of characters, so this is an overestimation, but sufficient for our purposes
        return max([len(d[2]) for d in data_loader])
    
    def create_index(self) -> None:
        items = []
        n_trunc = 0
        for _, target, text in iter(self._dataset(split=self.split)):
            result_dict = self.prepare_sentence_pair(sentence1=text)
            if result_dict['trunc']:
                n_trunc += 1

            items.append({'text': result_dict['input_ids'],
                          'attention_mask': result_dict['attention_mask'],
                          'target': target})
        
        self.log(f"Truncated {n_trunc} examples.")
        write_data_into_jsonl(items, os.path.join(self.path_to_data, self.get_index_files()[0]))
    

@register_dataset(name='sst_glue')
class SST(GLUE):
    def _dataset(self, split:str) -> torch.utils.data.Dataset:
        return torchtext.datasets.SST2(root=self.path_to_data, split=split)
    
    @property
    def _dataset_name(self):
        return 'sst'
    
    def _get_max_length(self, data_loader:Iterator) -> int:
        # iterate over the complete dataset would take unnecessarily long,
        # so just return the maximum sequence length
        return self.max_seq_len
    
    def create_index(self) -> None:
        items = []
        n_trunc = 0
        for text, target in iter(self._dataset(split=self.split)):
            result_dict = self.prepare_sentence_pair(sentence1=text)
            if result_dict['trunc']:
                n_trunc += 1

            items.append({'text': result_dict['input_ids'],
                          'attention_mask': result_dict['attention_mask'],
                          'target': target})
        
        self.log(f"Truncated {n_trunc} examples.")
        write_data_into_jsonl(items, os.path.join(self.path_to_data, self.get_index_files()[0]))
    

@register_dataset(name='qnli_glue')
class QNLI(GLUE):
    def _dataset(self, split:str) -> torch.utils.data.Dataset:
        return torchtext.datasets.QNLI(root=self.path_to_data, split=split)
    
    @property
    def _dataset_name(self):
        return 'qnli'
    
    def _get_max_length(self, data_loader:Iterator) -> int:
        return self.max_seq_len # see SST
    
    def create_index(self) -> None:
        items = []
        n_trunc = 0
        for target, text1, text2 in iter(self._dataset(split=self.split)):
            result_dict = self.prepare_sentence_pair(sentence1=text1, sentence2=text2)
            if result_dict['trunc']:
                n_trunc += 1

            items.append({'text': result_dict['input_ids'],
                          'attention_mask': result_dict['attention_mask'],
                          'token_type_ids': result_dict['token_type_ids'],
                          'target': target})
        
        self.log(f"Truncated {n_trunc} examples.")
        write_data_into_jsonl(items, os.path.join(self.path_to_data, self.get_index_files()[0]))
    

@register_dataset(name='rte_glue')
class RTE(QNLI): # from a logical point of view, RTE has nothing to do with QNLI, but the implementation is the same, so we can reuse the code
    def _dataset(self, split:str) -> torch.utils.data.Dataset:
        return torchtext.datasets.RTE(root=self.path_to_data, split=split)
    
    @property
    def _dataset_name(self):
        return 'rte'
    
    def _get_max_length(self, data_loader) -> int:
        # d[1] -> the sentence 1 as a string
        # d[2] -> the sentence 2 as a string
        # len(d[1]) + len(d[2]) -> the sum of the lengths of the strings
        # here we again overestimate the number of tokens
        return max([len(d[1])+len(d[2]) for d in data_loader])

@register_dataset(name='mrpc_glue')
class MRPC(RTE): # here the same as for RTE
    def _dataset(self, split:str) -> torch.utils.data.Dataset:
        return torchtext.datasets.MRPC(root=self.path_to_data, split=split)
    
    @property
    def _dataset_name(self):
        return 'mrpc'

@register_dataset(name='qqp_glue')
class QQP(GLUE):
    def _dataset(self, split:str) -> torch.utils.data.Dataset:
        # workaround, as this function will be used with iter(self._dataset(split=self.split)) in the constructor
        # since an iterator is expected, we just return an empty list here
        # we do not override the self._get_max_length() method, so the result of iter([]) will be used anyway, since
        # self._get_max_length() will just return self.max_seq_len
        # self._get_max_length() just returns self.max_seq_len, as it is faster than iterating over the complete dataset
        # would take unnecessarily long
        return []
    
    @property
    def _dataset_name(self):
        return 'qqp'
    
    def create_index(self) -> None:
        path = os.path.join(self.path_to_data, 'QQP', f'{self.split}.tsv') # path the the extracted file for the current split
        if not os.path.exists(path):
            URL='https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip'
            download_url(url=URL, root=self.path_to_data)
            filepath = os.path.join(self.path_to_data, os.path.basename(URL))
            with zipfile.ZipFile(filepath, 'r') as zip_ref:
                zip_ref.extractall(self.path_to_data)
            os.remove(filepath) # remove the zip file
    
        df = pd.read_csv(path, delimiter='\t')[['question1', 'question2', 'is_duplicate']]

        items = []
        n_trunc = 0
        for _, example in df.iterrows():
            result_dict = self.prepare_sentence_pair(sentence1=example['question1'], sentence2=example['question2'])
            if result_dict['trunc']:
                n_trunc += 1

            items.append({'text': result_dict['input_ids'],
                          'attention_mask': result_dict['attention_mask'],
                          'token_type_ids': result_dict['token_type_ids'],
                          'target': example['is_duplicate']})
        
        self.log(f"Truncated {n_trunc} examples.")
        write_data_into_jsonl(items, os.path.join(self.path_to_data, self.get_index_files()[0]))
    

@register_dataset(name='stsb_glue')
class STSB(RTE): # see RTE dataset for explanation of inheritance
    def _dataset(self, split:str) -> torch.utils.data.Dataset:
        return torchtext.datasets.STSB(root=self.path_to_data, split=split)
    
    @property
    def _dataset_name(self):
        return 'stsb'
    
    def _get_max_length(self, data_loader) -> int:
        return self.max_seq_len
    
    def _make_index(self) -> List[Dict[str, Any]]:
        items = []
        n_trunc = 0
        for _, target, text1, text2 in iter(self._dataset(split=self.split)):
            result_dict = self.prepare_sentence_pair(sentence1=text1, sentence2=text2)
            if result_dict['trunc']:
                n_trunc += 1

            items.append({'text': result_dict['input_ids'],
                          'attention_mask': result_dict['attention_mask'],
                          'token_type_ids': result_dict['token_type_ids'],
                          'target': target})
        
        self.log(f"Truncated {n_trunc} examples.")
            
        return items
    

@register_dataset(name='mnli_glue')
class MNLI(RTE): # see RTE dataset for explanation of inheritance
    def _dataset(self, split:str) -> torch.utils.data.Dataset:
        return torchtext.datasets.MNLI(root=self.path_to_data, split=split)
    
    def _get_max_length(self, data_loader:Iterator) -> int:
        return self.max_seq_len
    
    @property
    def _dataset_name(self):
        return 'mnli'

@register_dataset(name='wnli_glue')    
class WNLI(RTE): # see RTE dataset for explanation of inheritance
    def _dataset(self, split:str) -> torch.utils.data.Dataset:
        return torchtext.datasets.WNLI(root=self.path_to_data, split=split)
    
    def _get_max_length(self, data_loader:Iterator) -> int:
        return self.max_seq_len
    
    @property
    def _dataset_name(self):
        return 'wnli'

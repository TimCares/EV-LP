# @package _global_

hydra:
  run:
    dir: ${log_dir}

base_dir: /workspace
model_path: ${base_dir}/models
data_path: ${base_dir}
log_dir: ${base_dir}/logs
seed: 42

run_name: S-SMKE
id:
model_name: S-SMKE

load_checkpoint:

max_text_seq_len: 64 # inspired by BEiT3

imagenet_zero_shot_callback: True

# teacher
beit2_sd_path: ${model_path}/beitv2_base_patch16_224_pt1k.pth

model:
  embed_dim: 768
  depth: 6
  dropout: 0.0

data:
  dataloader:
    batch_size: 256
    num_workers: 8
    shuffle: True
    drop_last: True

  common: # keys used for all datasets
    data_path: ${data_path}
    max_seq_len: ${max_text_seq_len}

    train_transforms:
      - _name: "" # base transform, key for image data in dataset will just be "image", "<_name>_image" otherwise
        pretraining: True
        size: 224
        color_jitter: 0.4
        crop_scale: [0.9, 1.0]
    eval_transforms:
      - _name: "" # base transform, key for image data in dataset will just be "image", "<_name>_image" otherwise
        pretraining: True
        size: 224
    
    text_token_mask_prob: 0.0

  datamodules: # all datamodules do not have args that are specific to them, that is why we just provide an empty dict ("{}")
    COCOCaptions: {}

    ConceptualCaptions3m: {}

    ConceptualCaptions12m: {}

checkpoint:
  common:
    dirpath: ${model_path}/${run_name}
    enable_version_counter: False
    every_n_epochs: 1
    save_on_train_epoch_end: False # False -> run at end of validation
    verbose: True
    auto_insert_metric_name: False

  checkpoints:
    - monitor: val/loss
      mode: min
      filename: model-{step}-{${.monitor}:.4f}-val

    - monitor: train/loss
      mode: min
      filename: model-{step}-{${.monitor}:.4f}-train

optimizer:
  base_lr: 0.0001
  betas: [ 0.9,0.98 ]
  eps: 1e-06
  weight_decay: 0.01
  max_steps: 89273

lightning_trainer:
  default_root_dir: ${log_dir}
  accelerator: gpu
  devices: -1
  max_epochs: 7
  num_sanity_val_steps: 0 # would run zero shot at the beginning if > 0
  precision: 16-mixed
  log_every_n_steps: 50

  # deepspeed:
  #   stage: 2
  #   offload_optimizer: False
  #   allgather_bucket_size: 5e8 # size as recommended by pytorch lightning deepspeed docs
  #   reduce_bucket_size: 5e8 # size as recommended by pytorch lightning deepspeed docs

  ddp:
    gradient_as_bucket_view: True # optimizes memory usage
    static_graph: True # optimizes memory usage
